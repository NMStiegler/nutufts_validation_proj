{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Import things we'll need\n",
    "\n",
    "# Everything uses numpy\n",
    "import numpy as np\n",
    "\n",
    "# Needed for mlreco config\n",
    "import yaml\n",
    "\n",
    "# Tell script where mlreco and the data files are\n",
    "import sys\n",
    "SOFTWARE_DIR = \"/home/nstieg01/lartpc_mlreco3d\"\n",
    "sys.path.insert(0, SOFTWARE_DIR)\n",
    "\n",
    "# Import plotly for 3d plotting\n",
    "import plotly\n",
    "import plotly.graph_objs as go\n",
    "from plotly.offline import iplot, init_notebook_mode\n",
    "init_notebook_mode(connected=False)\n",
    "\n",
    "# Import mlreco tools\n",
    "from mlreco.visualization import scatter_points, plotly_layout3d\n",
    "from mlreco.visualization.gnn import scatter_clusters, network_topology, network_schematic\n",
    "from mlreco.utils.ppn import uresnet_ppn_type_point_selector\n",
    "from mlreco.utils.cluster.dense_cluster import fit_predict_np, gaussian_kernel\n",
    "from mlreco.main_funcs import process_config, prepare\n",
    "from mlreco.utils.gnn.cluster import get_cluster_label\n",
    "from mlreco.utils.deghosting import adapt_labels_numpy as adapt_labels\n",
    "from mlreco.visualization.gnn import network_topology\n",
    "\n",
    "# Import larcv\n",
    "from larcv import larcv\n",
    "\n",
    "# Import from sklearn for making a confusion matrix\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "\n",
    "# Import matplotlib for viewing confusion matrices\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copied in from lartpc_mlreco3d/config/train_ubmlreco_uresnet_ppn.cfg\n",
    "# We make sure:\n",
    "# training is false\n",
    "# batch_size is 1\n",
    "# shuffle is false\n",
    "# A seed is set in the sampler (so it does the same thing every time)\n",
    "# model_path is set to checkpoint_path\n",
    "# data_keys is set to data_path\n",
    "\n",
    "folder = \"/home/nstieg01/nWorking/validation_proj/\" # Directory to start with \n",
    "data_path = folder + \"mlrecodata_bnb_nu_0560.root\" # Name of data file\n",
    "checkpoint_path = folder + \"snapshot-619999.ckpt\" # Name of checkpoint file\n",
    "cfg = f'''\n",
    "# Reading in data\n",
    "iotool:\n",
    "  shuffle: False\n",
    "  num_workers: 1\n",
    "  collate_fn: CollateSparse\n",
    "  batch_size: 1\n",
    "  minibatch_size: 1\n",
    "  sampler:\n",
    "    batch_size: 1\n",
    "    minibatch_size: 1\n",
    "    name: SequentialBatchSampler\n",
    "    # seed: 1680813605 # Set a seed for repeatability\n",
    "  dataset:\n",
    "    name: LArCVDataset\n",
    "    data_keys:\n",
    "      - {data_path} # Files to load in\n",
    "    limit_num_files: 2000\n",
    "    #nvoxel_limit: 500000\n",
    "    nvoxel_limit: 800000\n",
    "    schema:\n",
    "      # Voxels\n",
    "      input_data:\n",
    "        parser: parse_sparse3d\n",
    "        args:\n",
    "          sparse_event_list:\n",
    "           - sparse3d_charge_plane0\n",
    "           - sparse3d_charge_plane1\n",
    "           - sparse3d_charge_plane2\n",
    "      # Labels - what is each voxel?\n",
    "      segment_label:\n",
    "        parser:  parse_sparse3d\n",
    "        args:\n",
    "          sparse_event_list:\n",
    "          - sparse3d_semantics_ghost\n",
    "      # Weights (something from training)\n",
    "      segment_weights:\n",
    "        parser:  parse_sparse3d\n",
    "        args:\n",
    "          sparse_event_list:\n",
    "          - sparse3d_semantics_weights\n",
    "      # ?\n",
    "      particles_label:\n",
    "        parser: parse_particle_points\n",
    "        args:\n",
    "          sparse_event: sparse3d_pcluster\n",
    "          particle_event: particle_corrected\n",
    "          include_point_tagging: True\n",
    "      # Which 'cluster' each voxel belongs to (like if they were made by the same particle)\n",
    "      pcluster:\n",
    "        parser: parse_sparse3d\n",
    "        args:\n",
    "          sparse_event_list:\n",
    "          - sparse3d_pcluster\n",
    "      # Whether a particle is cosmic in origin or from a neutrino interaction\n",
    "      cosmic_origin:\n",
    "        parser: parse_sparse3d\n",
    "        args:\n",
    "          sparse_event_list:\n",
    "          - sparse3d_cosmic_origin\n",
    "\n",
    "# Description of the model itself\n",
    "model:\n",
    "  name: uresnet_ppn_chain\n",
    "  modules:\n",
    "    uresnet_lonely:\n",
    "      #freeze_weight: True\n",
    "      num_classes: 5\n",
    "      num_input: 3\n",
    "      filters: 16\n",
    "      depth: 5\n",
    "      reps: 2\n",
    "      spatial_size: 3584\n",
    "      ghost: True\n",
    "      ghost_label: 5\n",
    "      weight_loss: True\n",
    "      weight_loss_power: 2.0\n",
    "      activation:\n",
    "        name: lrelu\n",
    "        args:\n",
    "          negative_slope: 0.01\n",
    "      allow_bias: False\n",
    "      weight_loss: False\n",
    "      norm_layer:\n",
    "        name: batch_norm\n",
    "        args:\n",
    "          eps: 0.0001\n",
    "          momentum: 0.01\n",
    "    ppn:\n",
    "      #freeze_weight: True\n",
    "      ppn_resolution: 1.0\n",
    "      mask_loss_name: 'BCE'\n",
    "      depth: 5\n",
    "      filters: 16\n",
    "      num_classes: 5\n",
    "      ppn_score_threshold: 0.6\n",
    "      spatial_size: 3584\n",
    "      classify_endpoints: True\n",
    "      particles_label_seg_col: -3\n",
    "      ghost: True\n",
    "  network_input:\n",
    "    - input_data\n",
    "  loss_input:\n",
    "    - segment_label\n",
    "    - particles_label\n",
    "    - segment_weights\n",
    "\n",
    "# Info for training & running\n",
    "trainval:\n",
    "  seed: 123\n",
    "  #unwrapper: unwrap_3d_mink\n",
    "  concat_result: ['seediness', 'margins', 'embeddings', 'fragments', 'fragments_seg', 'shower_fragments', 'shower_edge_index','shower_edge_pred','shower_node_pred','shower_group_pred','track_fragments', 'track_edge_index', 'track_node_pred', 'track_edge_pred', 'track_group_pred', 'particle_fragments', 'particle_edge_index', 'particle_node_pred', 'particle_edge_pred', 'particle_group_pred', 'particles','inter_edge_index', 'inter_node_pred', 'inter_edge_pred', 'node_pred_p', 'node_pred_type', 'flow_edge_pred', 'kinematics_particles', 'kinematics_edge_index', 'clust_fragments', 'clust_frag_seg', 'interactions', 'inter_cosmic_pred', 'node_pred_vtx', 'total_num_points', 'total_nonghost_points', 'spatial_embeddings', 'occupancy', 'hypergraph_features', 'features', 'feature_embeddings', 'covariance']\n",
    "  gpus: [] # Currently bugged, leaving empty is fine\n",
    "  weight_prefix: ./weights_trash/snapshot # Where weights get saved\n",
    "  model_path: {checkpoint_path} # Load the model from here\n",
    "  iterations: 10000000\n",
    "  report_step: 1\n",
    "  checkpoint_step: 10000\n",
    "  log_dir: ./log_trash # Where the log gets saved\n",
    "  train: False # Whether to train the model or not\n",
    "  debug: False\n",
    "  optimizer:\n",
    "    name: Adam\n",
    "    args:\n",
    "      lr: 0.001\n",
    "'''\n",
    "# Load the config as a yaml file\n",
    "cfg=yaml.load(cfg,Loader=yaml.Loader)\n",
    "# pre-process configuration (checks + certain non-specified default settings)\n",
    "process_config(cfg, verbose=False) # Make verbose true if you want to see what it parsed\n",
    "# prepare function configures necessary \"handlers\"\n",
    "hs=prepare(cfg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Num of entries to run on\n",
    "num_entries = len(hs.data_io)\n",
    "batch = 0 # Batchsize = 1\n",
    "\n",
    "# Setup numpy array to save\n",
    "to_save = []\n",
    "\n",
    "# Loop over all entries in file\n",
    "for entry in range(0, num_entries):\n",
    "    # Inform which entry\n",
    "    print(\"#############################\")\n",
    "    print(\"Running on entry:\", entry)\n",
    "    print(\"#############################\")\n",
    "    data, output = hs.trainer.forward(hs.data_io_iter)\n",
    "    print(data[\"index\"])\n",
    "#     # Setup to save everything from this event\n",
    "#     result = {}\n",
    "#     result[\"index\"] = data[\"index\"]\n",
    "#     result[\"file\"] = data_path\n",
    "    \n",
    "#     # Save easy info\n",
    "#     for key in output:\n",
    "#         contains = output[key][batch]\n",
    "#         key_type = type(contains)\n",
    "#         if (key_type == type(0.1) or key_type == type(1)):\n",
    "#             result[key] = contains\n",
    "    \n",
    "#     # Make confusion matrices\n",
    "#     input_data = data[\"input_data\"][batch]\n",
    "#     true_id_labels = data[\"segment_label\"][batch][:, -1]\n",
    "#     clusters = data['pcluster'][batch][:, 4]\n",
    "#     not_cosmic = data['cosmic_origin'][batch][:, 4] == 0\n",
    "#     cosmic = data['cosmic_origin'][batch][:, 4] == 1\n",
    "#     predicted_ghost_full = output['ghost'][batch]\n",
    "#     predicted_ghost = predicted_ghost_full.argmax(axis=1)\n",
    "#     predicted_id_labels_full = output['segmentation'][batch]\n",
    "#     predicted_id_labels = predicted_id_labels_full.argmax(axis=1)\n",
    "#     true_ghost_mask = true_id_labels == 5\n",
    "#     predicted_ghost_mask = predicted_ghost == 1\n",
    "#     full_predicted_labels = np.array([predicted_id_labels[i] if (predicted_ghost[i] != 1) else 5 for i in range(0, len(predicted_id_labels))])\n",
    "    \n",
    "#     # Set up the dictionary to put in result\n",
    "#     confusion_matrices = {}\n",
    "#     bins = [0, 1, 2, 3, 4, 5]\n",
    "#     confusion_matrices[\"bins\"] = bins\n",
    "#     confusion_matrices[\"labels\"] = [\"proton\", \"MIPs\", \"e$^-$/e$^+$/$\\gamma$\", \"$\\Delta$-ray\", \"Michel\", \"ghost\"]\n",
    "    \n",
    "    \n",
    "#     # Cosmics\n",
    "#     cm = confusion_matrix(true_id_labels[cosmic], full_predicted_labels[cosmic], labels=bins)\n",
    "#     confusion_matrices[\"cosmics\"] = cm\n",
    "    \n",
    "#     # Non-cosmics\n",
    "#     cm = confusion_matrix(true_id_labels[not_cosmic], full_predicted_labels[not_cosmic], labels=bins)\n",
    "#     confusion_matrices[\"non_cosmics\"] = cm\n",
    "    \n",
    "#     result[\"confusion_matrices\"] = confusion_matrices\n",
    "    \n",
    "#     # Save result\n",
    "#     to_save.append(result)\n",
    "    \n",
    "# Save to_save to a file\n",
    "print(\"Done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Move through data without evaluating it\n",
    "for i in range(16):\n",
    "    temp = next(hs.data_io_iter)\n",
    "    print(\"Index:\", temp[\"index\"][0], \"size:\", len(temp[\"input_data\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get number of entries, length of the data\n",
    "num_entries = len(hs.data_io)\n",
    "\n",
    "# Just for playing around with one\n",
    "entry = 0\n",
    "\n",
    "# get next entry using data_io_iter and then pass it through the network chain\n",
    "data, output = hs.trainer.forward(hs.data_io_iter)\n",
    "print(\"Size: \", len(data[\"input_data\"][0])) \n",
    "print(\"Index: \", data[\"index\"][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List the keys of the data dictionary\n",
    "data.keys()\n",
    "\n",
    "# input_data: The voxels themselves. Seven columns (indices 0-6). Column 0 is all 0s. 1-3 are voxel xyz coordinates\n",
    "#             4-6 are charges from planes 0, 1, 2\n",
    "input_data = data[\"input_data\"][entry]\n",
    "\n",
    "# segment_label: The truth voxel labels. Five columns. Column 0 is all 0s. 1-3 are voxel xyz coordinates. \n",
    "#                4 is the true label of that particle. 5 means it's really a ghost\n",
    "#\n",
    "# 0 = protons\n",
    "# 1 = MIPs (minimum ionizing particles like muon or pion)\n",
    "# 2 = EM showers (electron, positron, photon)\n",
    "# 3 = Delta ray electrons (hard scattering off of charged particles)\n",
    "# 4 = Michel electrons (decay of muons)\n",
    "true_id_labels = data[\"segment_label\"][entry][:, -1]\n",
    "\n",
    "# segment_weights: Five columns. Col 0 is all 0s. 1-3 are voxel xyz. 4 gives most particles weight 1 but some weight 2.\n",
    "#                  Not sure why. Something from training. \n",
    "\n",
    "# particles_label: 7 columns of 59 rows? Not sure what's going on here. Col 0 is all 0s. \n",
    "\n",
    "# pcluster: 5 cols. Col 0 is all 0s. Cols 1-3 are voxel xyz. Col 4 gives which 'cluster' each voxel belongs to\n",
    "clusters = data['pcluster'][entry][:, 4]\n",
    "\n",
    "# cosmic_origin: 5 cols. Col 0 is all 0s. Cols 1-3 are voxel xyz. Col 4 gives '1' if the voxel is part of cosmic background\n",
    "#                and '0' if it's not\n",
    "not_cosmic = data['cosmic_origin'][0][:, 4] == 0\n",
    "cosmic = data['cosmic_origin'][0][:, 4] == 1\n",
    "\n",
    "print(\"Number of non-cosmics:\", sum(not_cosmic), '/', len(not_cosmic), '=',round(sum(not_cosmic) / len(not_cosmic) * 100, 3), '% of voxels')\n",
    "\n",
    "# index: Just a number. Not sure what/why"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List the keys of the output dictionary\n",
    "output.keys()\n",
    "\n",
    "# ghost: The network's prediction of whether something is a ghost point or not. Two columns. Whichever column is\n",
    "#        greater tells us its prediction. If the second column (index 1) is greater, it predicts it's a ghost,\n",
    "#        otherwise not\n",
    "predicted_ghost_full = output['ghost'][entry]\n",
    "predicted_ghost = predicted_ghost_full.argmax(axis=1)\n",
    "\n",
    "# segmentation: The network's prediction of what each voxel is (even does predictions for ghost points). \n",
    "#               Five columns. One for each label. The index of the column is the label it's predicting.\n",
    "#               Whichever column has the highest value in it is the column it's predicting.\n",
    "predicted_id_labels_full = output['segmentation'][entry]\n",
    "predicted_id_labels = predicted_id_labels_full.argmax(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "result = {}\n",
    "for key in output:\n",
    "    contains = output[key][0]\n",
    "    key_type = type(contains)\n",
    "    if (key_type == type(0.1) or key_type == type(1)):\n",
    "        result[key] = contains\n",
    "\n",
    "for key in result:\n",
    "    print(f\"{key}: {result[key]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make masks of whether it's really a ghost or predicted to be one\n",
    "true_ghost_mask = true_id_labels == 5\n",
    "predicted_ghost_mask = predicted_ghost == 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Confusion matrix for ghost points on all voxels\n",
    "bins = [True, False]\n",
    "cm = confusion_matrix(true_ghost_mask, predicted_ghost_mask, labels=bins, normalize='true')\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=[\"Ghost\", \"Nonghost\"])\n",
    "disp.plot()\n",
    "plt.title(\"Is a particle a ghost point?\\n(all voxels)\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confusion Matrix for ghost points on non-cosmic voxels\n",
    "bins = [True, False]\n",
    "cm = confusion_matrix(true_ghost_mask[not_cosmic], predicted_ghost_mask[not_cosmic], labels=bins, normalize='true')\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=[\"Ghost\", \"Nonghost\"])\n",
    "disp.plot()\n",
    "plt.title(\"Is a particle a ghost point?\\n(only non-cosmic voxels)\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make a mask of just the indices which are not ghost points (label != 5)\n",
    "true_nonghost_indices = true_id_labels < 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Confusion matrix on real non-ghost voxels\n",
    "bins = [0, 1, 2, 3, 4]\n",
    "cm = confusion_matrix(true_id_labels[true_nonghost_indices], predicted_id_labels[true_nonghost_indices], labels=bins, normalize='true')\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=[\"proton\", \"MIPs\", \"e$^-$/e$^+$/$\\gamma$\", \"$\\Delta$-ray\", \"Michel\"])\n",
    "disp.plot()\n",
    "plt.title(\"What particle is it?\\nOnly real non-ghost voxels\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find how many non-cosmic voxels are ghost points \n",
    "not_cosmic_ghosts = not_cosmic & true_ghost_mask\n",
    "not_cosmic_not_ghosts = not_cosmic & ~true_ghost_mask\n",
    "num_not_cosmic_ghosts = sum(not_cosmic_ghosts)\n",
    "num_not_cosmic = sum(not_cosmic)\n",
    "print(f\"There are {num_not_cosmic_ghosts} voxels which are not cosmic and also ghost points. {num_not_cosmic_ghosts} / {num_not_cosmic} = {round((num_not_cosmic_ghosts / num_not_cosmic) * 100 , 3)}% ghost points \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confusion matrix on real non-ghost voxels for non-cosmic voxels\n",
    "bins = [0, 1, 2, 3, 4]\n",
    "cm = confusion_matrix(true_id_labels[not_cosmic_not_ghosts], predicted_id_labels[not_cosmic_not_ghosts], labels=bins, normalize='true')\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=[\"proton\", \"MIPs\", \"e$^-$/e$^+$/$\\gamma$\", \"$\\Delta$-ray\", \"Michel\"])\n",
    "disp.plot()\n",
    "plt.title(\"What particle is it?\\nOnly real non-ghost, non-cosmic voxels\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confusion matrix on voxels the network thinks are non-ghost\n",
    "bins = [0, 1, 2, 3, 4, 5]\n",
    "cm = confusion_matrix(true_id_labels[~predicted_ghost_mask], predicted_id_labels[~predicted_ghost_mask], labels=bins)\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=[\"proton\", \"MIPs\", \"e$^-$/e$^+$/$\\gamma$\", \"$\\Delta$-ray\", \"Michel\", \"ghost\"])\n",
    "disp.plot()\n",
    "plt.title(\"What particle is it?\\nPoints network thinks are non-ghost\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get voxels network thinks are non-ghost and are really non-cosmic\n",
    "#ncpng is an acronym for not_cosmic_predicted_not_ghost\n",
    "ncpng = not_cosmic_predicted_not_ghost = not_cosmic & ~predicted_ghost_mask\n",
    "ncpng_kept = sum(ncpng)\n",
    "print(f\"There are {ncpng_kept} voxels predicted not ghost and really not cosmic. {ncpng_kept} / {len(ncpng)} = {round((ncpng_kept/len(ncpng)) * 100, 3)} %\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make a confusion matrix for voxels network thinks are non-ghost and are really non-cosmic\n",
    "# ncpng = not_cosmic_predicted_not_ghost\n",
    "bins = [0, 1, 2, 3, 4, 5]\n",
    "cm = confusion_matrix(true_id_labels[ncpng], predicted_id_labels[ncpng], labels=bins)\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=[\"proton\", \"MIPs\", \"e$^-$/e$^+$/$\\gamma$\", \"$\\Delta$-ray\", \"Michel\", \"ghost\"])\n",
    "disp.plot()\n",
    "plt.title(\"What particle is it?\\nNon-cosmic points network thinks are non-ghost\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predicted_ghost\n",
    "# predicted_id_labels\n",
    "# true_id_labels\n",
    "# not_cosmic\n",
    "# cosmic\n",
    "\n",
    "full_predicted_labels = np.array([predicted_id_labels[i] if (predicted_ghost[i] != 1) else 5 for i in range(0, len(predicted_id_labels))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "bins = [0, 1, 2, 3, 4, 5]\n",
    "cm = confusion_matrix(true_id_labels, full_predicted_labels, labels=bins)\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=[\"proton\", \"MIPs\", \"e$^-$/e$^+$/$\\gamma$\", \"$\\Delta$-ray\", \"Michel\", \"ghost\"])\n",
    "disp.plot()\n",
    "plt.title(\"What particle is it?\\nAll points\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bins = [0, 1, 2, 3, 4, 5]\n",
    "cm = confusion_matrix(true_id_labels[not_cosmic], full_predicted_labels[not_cosmic], labels=bins)\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=[\"proton\", \"MIPs\", \"e$^-$/e$^+$/$\\gamma$\", \"$\\Delta$-ray\", \"Michel\", \"ghost\"])\n",
    "disp.plot()\n",
    "plt.title(\"What particle is it?\\nNot cosmic\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bins = [0, 1, 2, 3, 4, 5]\n",
    "cm = confusion_matrix(true_id_labels[cosmic], full_predicted_labels[cosmic], labels=bins)\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=[\"proton\", \"MIPs\", \"e$^-$/e$^+$/$\\gamma$\", \"$\\Delta$-ray\", \"Michel\", \"ghost\"])\n",
    "disp.plot()\n",
    "plt.title(\"What particle is it?\\nCosmic\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## For plotting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# WE ARE USING LARCV1 (the submodule included in the ICDL repository)\n",
    "import ROOT as rt\n",
    "from larcv import larcv\n",
    "from larlite import larlite\n",
    "from larflow import larflow\n",
    "from ROOT import larutil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SETUP GEOMETRY: FOR VISUALIZATION\n",
    "DETECTOR = \"uboone\"\n",
    "\n",
    "if DETECTOR == \"icarus\":\n",
    "    detid = larlite.geo.kICARUS\n",
    "elif DETECTOR == \"uboone\":\n",
    "    detid = larlite.geo.kMicroBooNE\n",
    "elif DETECTOR == \"sbnd\":\n",
    "    detid = larlite.geo.kSBND\n",
    "    \n",
    "larutil.LArUtilConfig.SetDetector(detid)\n",
    "\n",
    "import lardly\n",
    "from lardly.detectoroutline import get_tpc_boundary_plot\n",
    "\n",
    "tpclines = get_tpc_boundary_plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trace = []\n",
    "\n",
    "ADC_MAX = 100.0\n",
    "\n",
    "# What is really a ghost point\n",
    "# trace+= scatter_points(input_data[true_id_labels == 5],markersize=1,color=input_data[true_id_labels == 5, -2], cmin=0,cmax=ADC_MAX)\n",
    "# trace[-1].name = 'input_data (true ghost points)'\n",
    "# trace[-1].marker.colorscale='viridis'\n",
    "\n",
    "# fig = go.Figure(data=trace,layout=plotly_layout3d())\n",
    "# fig.update_layout(legend=dict(x=1.0, y=0.8))\n",
    "\n",
    "# iplot(fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trace = []\n",
    "\n",
    "ADC_MAX = 100.0\n",
    "\n",
    "# What the network thinks are ghost points\n",
    "# trace+= scatter_points(input_data[predicted_ghost_mask],markersize=1,color=input_data[predicted_ghost_mask, -2], cmin=0,cmax=ADC_MAX)\n",
    "# trace[-1].name = 'Network thinks are ghost'\n",
    "# trace[-1].marker.colorscale='viridis'\n",
    "\n",
    "# fig = go.Figure(data=trace,layout=plotly_layout3d())\n",
    "# fig.update_layout(legend=dict(x=1.0, y=0.8))\n",
    "\n",
    "# iplot(fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trace = []\n",
    "\n",
    "ADC_MAX = 100.0\n",
    "\n",
    "# All real points colored via their cluster\n",
    "# trace+= scatter_points(input_data[true_nonghost_indices],markersize=1,color=clusters[true_nonghost_indices]) # , cmin=0,cmax=ADC_MAX)\n",
    "# trace[-1].name = 'All real points & Cluster'\n",
    "# # trace[-1].marker.colorscale='viridis'\n",
    "\n",
    "# fig = go.Figure(data=trace,layout=plotly_layout3d())\n",
    "# fig.update_layout(legend=dict(x=1.0, y=0.8))\n",
    "\n",
    "# iplot(fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trace = []\n",
    "\n",
    "ADC_MAX = 100.0\n",
    "\n",
    "# Not Ghost\n",
    "# trace+= scatter_points(input_data[true_nonghost_indices],markersize=1,color=input_data[true_nonghost_indices, -2], cmin=0,cmax=ADC_MAX)\n",
    "# trace[-1].name = 'Real Not Ghost'\n",
    "# trace[-1].marker.colorscale='viridis'\n",
    "\n",
    "# fig = go.Figure(data=trace,layout=plotly_layout3d())\n",
    "# fig.update_layout(legend=dict(x=1.0, y=0.8))\n",
    "\n",
    "# iplot(fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trace = []\n",
    "\n",
    "ADC_MAX = 100.0\n",
    "\n",
    "# Non-Cosmic points\n",
    "# trace+= scatter_points(input_data[not_cosmic],markersize=1,color=input_data[not_cosmic, -2], cmin=0,cmax=ADC_MAX)\n",
    "# trace[-1].name = 'Non-cosmics'\n",
    "# trace[-1].marker.colorscale='viridis'\n",
    "\n",
    "# fig = go.Figure(data=trace,layout=plotly_layout3d())\n",
    "# fig.update_layout(legend=dict(x=1.0, y=0.8))\n",
    "\n",
    "# iplot(fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "half_mask = [True if num % 2 == 0 else False for num in range(len(input_data))]\n",
    "\n",
    "color_map = np.array([10 if cos else 70 for cos in cosmic])\n",
    "\n",
    "trace = []\n",
    "\n",
    "ADC_MAX = 100.0\n",
    "\n",
    "\n",
    "\n",
    "# Half Data -- colored by cosmic or not\n",
    "# trace+= scatter_points(input_data[half_mask],markersize=1,color=color_map[half_mask], cmin=0,cmax=ADC_MAX)\n",
    "# trace[-1].name = 'everything'\n",
    "# trace[-1].marker.colorscale='viridis'\n",
    "\n",
    "# fig = go.Figure(data=trace,layout=plotly_layout3d())\n",
    "# fig.update_layout(legend=dict(x=1.0, y=0.8))\n",
    "\n",
    "# iplot(fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trace = []\n",
    "\n",
    "ADC_MAX = 100.0\n",
    "\n",
    "\n",
    "\n",
    "# All points\n",
    "# trace+= scatter_points(input_data[:],markersize=1,color=input_data[:, -2], cmin=0,cmax=ADC_MAX)\n",
    "# trace[-1].name = 'everything'\n",
    "# trace[-1].marker.colorscale='viridis'\n",
    "\n",
    "# fig = go.Figure(data=trace,layout=plotly_layout3d())\n",
    "# fig.update_layout(legend=dict(x=1.0, y=0.8))\n",
    "\n",
    "# iplot(fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trace = []\n",
    "\n",
    "ADC_MAX = 100.0\n",
    "\n",
    "# Just Cosmics\n",
    "# trace+= scatter_points(input_data[cosmic], markersize=1,color=input_data[cosmic, -2], cmin=0,cmax=ADC_MAX)\n",
    "# trace[-1].name = 'cosmics'\n",
    "# trace[-1].marker.colorscale='viridis'\n",
    "\n",
    "# fig = go.Figure(data=trace,layout=plotly_layout3d())\n",
    "# fig.update_layout(legend=dict(x=1.0, y=0.8))\n",
    "\n",
    "# iplot(fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trace = []\n",
    "\n",
    "ADC_MAX = 4\n",
    "\n",
    "# Non-Cosmic points colored according to their true label\n",
    "# trace+= scatter_points(input_data[not_cosmic],markersize=1,color=true_id_labels[not_cosmic], cmin=3,cmax=ADC_MAX)\n",
    "# trace[-1].name = 'Non-cosmics'\n",
    "# trace[-1].marker.colorscale='viridis'\n",
    "\n",
    "# fig = go.Figure(data=trace,layout=plotly_layout3d())\n",
    "# fig.update_layout(legend=dict(x=1.0, y=0.8))\n",
    "\n",
    "# iplot(fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('scriptoutput.out.npy', 'rb') as f:\n",
    "    a = np.load(f, allow_pickle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(a[0][\"confusion_matrices\"][\"cosmics\"])\n",
    "print(a[0][\"confusion_matrices\"][\"cosmics\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(a[0][\"confusion_matrices\"].keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"hi\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
